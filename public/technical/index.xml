<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Technical on Perpetual White Belt</title>
    <link>https://www.calmdownkarm.com/technical/</link>
    <description>Recent content in Technical on Perpetual White Belt</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Tue, 11 Jun 2019 09:00:00 +0630</lastBuildDate>
    
	<atom:link href="https://www.calmdownkarm.com/technical/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Lessons from a weekend of failing to handle a non-trivial amount of text</title>
      <link>https://www.calmdownkarm.com/technical/text-lessons/</link>
      <pubDate>Tue, 11 Jun 2019 09:00:00 +0630</pubDate>
      
      <guid>https://www.calmdownkarm.com/technical/text-lessons/</guid>
      <description>I&#39;ve been playing around with Common Crawl data lately - training some hindi language models. So I had about 25~ gigs of text to process, which I suppose is tiny compared to English Common Crawl (in the TB range); but it&#39;s right on the cusp of having to think a little bit on how to deal with it in the most efficient manner.
The server I was running this on had some 40 odd CPU cores and like 400 Gigs of RAM, so I wasn&#39;t terribly worried.</description>
    </item>
    
    <item>
      <title>Machine Learning is NOT Math</title>
      <link>https://www.calmdownkarm.com/technical/ml-not-math/</link>
      <pubDate>Fri, 31 May 2019 09:00:00 +0630</pubDate>
      
      <guid>https://www.calmdownkarm.com/technical/ml-not-math/</guid>
      <description>Machine Learning is firmly grounded in math - but why does it never feel like that day to day?. When I sit down to solve a math problem, not only is there a definitive answer, but more often than not there&#39;s an ideal way to approach the question. This ideal way never changes. When I sit down to solve an ML problem on the other hand, I always have to go through the same steps-</description>
    </item>
    
    <item>
      <title>डेटा डेजर्ट</title>
      <link>https://www.calmdownkarm.com/technical/data-deserts/</link>
      <pubDate>Tue, 16 Apr 2019 09:00:00 +0630</pubDate>
      
      <guid>https://www.calmdownkarm.com/technical/data-deserts/</guid>
      <description>The other day, I was helping out some people at my research lab write a couple of sentiment analysis classifiers for the Hindi Language and I was surprised to find that almost every famous English Language Model(Glove, FastText, Word2vec) has a Hindi equivalent that is very easy to find. I was even more surprised to find despite this, there is a dearth of well put together, annotated datasets for modern NLP Tasks.</description>
    </item>
    
    <item>
      <title>Healthy Dose of Skepticism with Machine Learning</title>
      <link>https://www.calmdownkarm.com/technical/skepticism/</link>
      <pubDate>Tue, 04 Dec 2018 09:00:00 +0630</pubDate>
      
      <guid>https://www.calmdownkarm.com/technical/skepticism/</guid>
      <description>This week, 2 small changes to the way I built two models resulted in an absurd boost of performance of both. To the extent where it doesn&#39;t even make sense for the models to be this good. The obvious conclusion then becomes that I&#39;ve made some mistake somewhere. Thing is, I have absolutely no idea where the mistake lies - but in the attempt to figure it out, I was forced to take a closer look at both the model and the data, aided by a healthy dose of skeptcisim towards what the models do and how good their performance can be.</description>
    </item>
    
    <item>
      <title>A twitter bot under 20 lines of code and 30 lines of configuration</title>
      <link>https://www.calmdownkarm.com/technical/twitterbot/</link>
      <pubDate>Tue, 16 Oct 2018 09:00:00 +0630</pubDate>
      
      <guid>https://www.calmdownkarm.com/technical/twitterbot/</guid>
      <description>A project underway at the lab in IIITDelhi that I&#39;m volunteering at involves collecting a decently large collection of tweets. The bot needs to run at regular intervals, hit the search API with a series of keywords and store the returned tweets. More than that, it needed to be written completely in python and fairly difficult for a bored undergrad to break. I chose Gramex - since I work for the company that builds it, and I hadn&#39;t had a chance to test our TwitterRESTHandler yet.</description>
    </item>
    
    <item>
      <title>SSH Configurations</title>
      <link>https://www.calmdownkarm.com/technical/ssh-config/</link>
      <pubDate>Sun, 16 Sep 2018 09:00:00 +0630</pubDate>
      
      <guid>https://www.calmdownkarm.com/technical/ssh-config/</guid>
      <description>I&#39;m spending most of this long discontinuous weekend to run through all the fast.ai notebooks - while I find this course really helped me get a theoretical understanding of what neural nets are and how they work, I find that it takes me an inordinate amount of time to produce actual code - probably due to the fact that I never wrote anything while doing the course.
So this time around, I&#39;m simply recreating each of the notebooks as quickly as I can, but ensuring that I&#39;m actually implementing everything myself as compared to just running cells in their notebooks</description>
    </item>
    
    <item>
      <title>Convolutional Neural Networks for Sentence Classification</title>
      <link>https://www.calmdownkarm.com/technical/yoon-kim/</link>
      <pubDate>Fri, 10 Aug 2018 09:00:00 +0630</pubDate>
      
      <guid>https://www.calmdownkarm.com/technical/yoon-kim/</guid>
      <description>This post is going to be a small repository of information about Yoon Kim&#39;s paper on using Convolutional Neural Networks for Sentence Classification.
  Arxiv Link
  Author&#39;s Github Link
  TODO: My pytorch implementation
  Yoon Kim and his team, talk about using Convolutional Neural Nets as a means for sentence classification - essentially, they create sentence vectors by taking word vectors and concatenating them together, then pass them through layers of convolutions or filters to create what they call a feature map, which is literally just a set of filter outputs.</description>
    </item>
    
  </channel>
</rss>