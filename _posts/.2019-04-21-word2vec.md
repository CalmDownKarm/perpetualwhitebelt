---
title: Word2Vec
date: 2019-04-21
draft: true
layout: post
categories: tech, data science, machine learning, nlp, word2vec
---

It's a little late to be talking about word2vec, it's 2019, Mikolov's paper is roughly 6 years old now, and by and large people have moved on from 1 dimensional language models to much more sophisticated RNNs with self attention or transformers trained on insane amounts of compute and data. The canonical example of King - Man + Woman = Queen is pretty much a byword in nlp today. So why talk about them? Well, I think they're pretty cool. They're fairly dependable tech, and because they're essentially based on counting which words appear close to each most frequently, they're significantly more interpretable than some of the newer, fancier techniques. Plus the idea of word2vec essentially being a sequence counter can and has been used in some pretty cool ways that I don't think BERT or GPT2 can do. 

So let's start with the basics - NLP deals with figuring out how to get computers to understand and generate what we call natural language; what people speak, write and listen to every day. A big part of the problem is that words have multiple meanings, completely different words can have very similar, related meanings and vice versa. What compounds the issue is that so much of language is context dependent, thanks to cultural subtext and sarcasm, it becomes very difficult to figure it out. In the past, people tried to create large [relational databases](https://wordnet.princeton.edu/) of words. Not relational as in SQL, but in the manner that the word chair is related to furniture and both are related to object. Given a body of text, you could look up the heirarchial databased and figure out which words were similar to each other. There are undoubtedly a couple of problems with this approach; whether two words are related or not, becomes completely binary - they either are or they arent. Consider words like `good`, `great` and `marvellous`; is good closer/more related to great or marvellous? Essentially, we want to figure out how to make the difference between any two otherwise discrete words to be a continous value. Well, people in math and physics have known how to convert discrete things into continous things for a super long time - they're called vectors. In school, we were introduced to the difference between speed and velocity; speed is a scalar which quantifies how fast something is moving, velocity is perhaps more meaningful - it quantifies how fast something is moving, along with which direction it's moving in. Moreover, given that an object is moving in a particular direction - it's velocity at any point of space is a vector that combines velocities along all 3 fundamental dimensions. The simplest way to achieve something similar with words is to just say, well, for every word we know, we'll have a unique representation, so if we have `n` words - our vocabularly can just be a `nXn` matrix where every row represents a single word, thus every word gets mapped to a number, and the numbers themselves are represented as `one-hot-vectors`: a sequence of zeroes with a 1 in a single position. 
This works, we have a unique way to represent each word, however this has the same problem we had earlier - all the vectors are orthogonal - every word is different from any other word, and so `Vgood * Vgreat` = 0 and `Vgood * Vmarvellous` is also zero. Also, given that there are millions of words in most languages, we have these huge matrices of words which are all just a whole bunch of zeroes, there isn't that much information encoded there. 

To improve upon this representation, we make a simple, but very powerful assumption; that similar words will appear close to each other inside documents. Working from this, we can just count how often words appear. This time, we have a nXm matrix, of n words and m documents, every time we see a word in a document, we just increment the respective position in the matrix and then essentially, we have a matrix of word counts.  If I sum up any row, I get the total number of times a word appeared across all my documents, and if I sum up any column, I have the total number of words that appeared in that document. or vice versa for that matter, the orientation of the matrix isn't that important. At this point, I have a frequency distribution of sorts - I know which words appear the most frquently, around which other sets of words they appear and because of my assumption, I know which of them are related. This has a huge problem though - as the number of documents increase, the matrix gets larger even though the number of words doesn't change too much. Eventually, I have a huge monolithic matrix of the same n million rows of words, but billions of columns. 

So if we think about it, we only really care about words that occur near each other - so as long as we only track words within a window size our matrix becomes a lot smaller. so if I have the following sentences as my documents - 

* I love star wars
* I like movies
* star wars is better than star trek

then depending on a window size, I can create co-occurence matrices where I have [I, love, star, wars, movies, like, is, better, than, trek] on both axes, and I can count how often a word occurs within that window size. If I then divide by the number of times each word appears overall, then I suddenly get the N-gram language models that were used before deep learning became a major thing. 
That is, once we have this matrix of words and their counts, we can figure out the probability of seeing any sequence of words, just by counting their occurence in this matrix. In the sentence `see I have a rhyme repeating`, 
In a unigram model - where my window around each word is 1, P('see I have  rhyme repeating') is just P(see)*P(I)*P(a)... however in a bigram model, P('see I have a rhyme repeating') is P(see)*P(I|see)*P(have|I)...P(repeating|rhyme) again, these probabilities are calculated simply by counting, so P(I|see) is the count of the times we saw the word I right after the word see divided by the count of the total number of times we saw I.
Instead, if we then use some sort of dimensionality reduction - SVD, or Matrix Factorization or something similar, we can then look at singular vectors in this matrix, and depending upon the level of dimensionality we want, have  k dimensional vectors that encode how likely a word is to appear around other words - that is how likely a word is to appear in a certain context. Once this information is encoded, surprisingly enough they magically begin to encode relationships (I'm sure this wasn't as surprising to Mikolov, but it definitely blew my mind) Typically, we'd be using 300 dimensions or higher. 

In the word2vec algorithm in particular, we start with this idea of the N-gram languge model,  
With this language model, we can now treat all our documents is a 1 gigantic continous document, and slice through it in a sliding window. If I have a 5 word window, then for every word in the window, I want to maximise the likelihood or probabilbilty of getting that word having seen a context word - so in the window `see I have a rhyme repeating` I would want to maximise the likelihood of P(I|see) and P(have|see) and P(rhyme|see), subsequently I would then want to maximise the likelihood of P(see|I) and P(have|I) and P(a|I) and P(rhyme|I) and so on. The goal of doing this is to get vectors out right, so in these examples, imagine `see` and `I` and `a` aren't the english words themselves but are randomly initialized vectors of the words. and in actuality, we're seeing each word in 2 ways, once as a central word and once as a context word, so we actually create 2 randomly initialized vectors for each word, Vout for the context word and Vin for the central word. so P(Vout|Vin) is actually P(see|I). 


Allison Parrish has this [great video](https://youtu.be/L3D0JEA1Jdc?t=940) in which she talks about her experiments with word vectors, but when you plot these k dimensional vectors as indvidual radial plots and you can very quickly see which word vectors are similar to each other just by looking at the shape of the plot. But the idea is pretty simple - by aggregating a bunch of these words in contexts, words that mean similar things or are used in very similar contexts end up as very similar vectors. From this word soup, the famous analogies emerge; stuff like dog is to puppy as cat is to kitten shows up. 



Some really cool stuff that's been done with word vectors
 - [Allison Parrish's explorations of semantic spaces](opentranscripts.org/transcript/semantic-space-literal-robots/)
 - [Playlist2Vec](https://github.com/mattdennewitz/playlist-to-vec)
 - [Graph2Vec](https://arxiv.org/abs/1707.05005)
 - [Creator of Gensim's word vectors tutorial](https://rare-technologies.com/word2vec-tutorial/)
 - 